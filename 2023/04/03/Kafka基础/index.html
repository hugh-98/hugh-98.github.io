<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>Kafka基础——入门 | Hugh的个人博客</title><meta name="author" content="Hugh"><meta name="copyright" content="Hugh"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="全文参考尚硅谷视频 Kafka 概述 定义 Kafka 传统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。 Kafka 最新定义： Kafka 是一个开源的分布式事件流平台（ Event Streaming">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka基础——入门">
<meta property="og:url" content="https://hugh-98.github.io/2023/04/03/Kafka%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="Hugh的个人博客">
<meta property="og:description" content="全文参考尚硅谷视频 Kafka 概述 定义 Kafka 传统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。 Kafka 最新定义： Kafka 是一个开源的分布式事件流平台（ Event Streaming">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hugh-98.github.io/img/avatar.jpg">
<meta property="article:published_time" content="2023-04-03T11:46:27.000Z">
<meta property="article:modified_time" content="2023-04-05T12:03:03.106Z">
<meta property="article:author" content="Hugh">
<meta property="article:tag" content="Kafka入门">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hugh-98.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://hugh-98.github.io/2023/04/03/Kafka%E5%9F%BA%E7%A1%80/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?75e27ce1a302654cd332f061e9a5e29b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Hugh","link":"链接: ","source":"来源: Hugh的个人博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Kafka基础——入门',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-04-05 20:03:03'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url('/img/default.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Hugh的个人博客"><span class="site-name">Hugh的个人博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Kafka基础——入门</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-04-03T11:46:27.000Z" title="发表于 2023-04-03 19:46:27">2023-04-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-04-05T12:03:03.106Z" title="更新于 2023-04-05 20:03:03">2023-04-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Kafka/">Kafka</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">17.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>71分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Kafka基础——入门"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>全文参考<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1vr4y1677k">尚硅谷视频</a></p>
<h2 id="Kafka-概述">Kafka 概述</h2>
<h3 id="定义">定义</h3>
<p><strong>Kafka 传统定义</strong>：Kafka是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。</p>
<p><strong>发布/订阅</strong>：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。</p>
<p><strong>Kafka 最新定义</strong>： Kafka 是一个开源的<strong>分布式事件流平台</strong>（ Event Streaming Platform），被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。</p>
<h3 id="消息队列">消息队列</h3>
<p>目前企业中比较常见的消息队列产品主要有Kafka 、ActiveMQ、RabbitMQ、RocketMQ 等。</p>
<p><strong>在大数据场景主要采用Kafka 作为消息队列</strong>。在JavaEE 开发中主要采用ActiveMQ、RabbitMQ、RocketMQ。</p>
<h4 id="传统消息队列的应用场景">传统消息队列的应用场景</h4>
<p>传统的消息队列的主要应用场景包括：<strong>缓存/消峰、解耦和异步通信</strong>。</p>
<ul>
<li>缓冲/消峰：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941394.png" alt="image-20221004102155950"></p>
<ul>
<li>解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941395.png" alt="image-20221004102209535"></p>
<ul>
<li>异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941396.png" alt="image-20221004102232959"></p>
<h4 id="消息队列的两种模式">消息队列的两种模式</h4>
<ul>
<li>点对点模式
<ul>
<li>消费者主动拉取数据，消息收到后清除消息</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941397.png" alt="image-20221004102414472"></p>
<ul>
<li>发布/订阅模式
<ul>
<li>可以有多个topic主题（浏览、点赞、收藏、评论等）</li>
<li>消费者消费数据之后，不删除数据</li>
<li>每个消费者相互独立，都可以消费到数据</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941398.png" alt="image-20221004102437824"></p>
<h3 id="Kafka-基础架构">Kafka 基础架构</h3>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941399.png" alt="image-20221004102705430"></p>
<ul>
<li>Producer：消息生产者，就是向Kafka broker 发消息的客户端。</li>
<li>Consumer：消息消费者，向Kafka broker 取消息的客户端。</li>
<li>Consumer Group（CG）：消费者组，由多个consumer 组成。<strong>同一个消费者组内的每个消费者负责消费不同分区的数据，一个分区只能由同一个组内的一个消费者消费；消费者组之间互不影响</strong>。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
<li>Broker：一台Kafka 服务器就是一个broker。一个集群由多个broker 组成。一个broker 可以容纳多个topic。</li>
<li>Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic。</li>
<li>Partition：为了实现扩展性，一个非常大的topic 可以分布到多个broker（即服务器）上，一个topic 可以分为多个partition，<strong>每个partition 是一个有序的队列</strong>。正常来说，一个分区只对应一个Broker</li>
<li>Replica：副本。一个topic 的<strong>每个分区都有若干个副本</strong>，一个Leader 和若干个Follower。</li>
<li>Leader：每个分区多个副本的“主”，<strong>生产者发送数据的对象，以及消费者消费数据的对象都是Leader</strong>。</li>
<li>Follower：每个分区多个副本中的“从”，实时从Leader 中同步数据，保持和Leader 数据的同步。Leader 发生故障时，某个Follower 会成为新的Leader。</li>
</ul>
<blockquote>
<ul>
<li>在 同一个消费者组内，一个 Partition  只能被 一个消费者消费</li>
<li>在 同一个消费者组内，所有消费者  组合起来必定可以消费一个 Topic 下的所有 Partition</li>
<li>在 同一个消费组内，一个消费者 可以消费多个 Partition 的信息</li>
<li>在 不同消费者组内，同一个分区 可以被 多个消费者消费</li>
<li><strong>每个消费者组一定会完整消费一个 Topic 下的所有 Partition</strong></li>
<li>topic是逻辑的概念，partition是物理的概念</li>
<li>Kafka 的消费模式和发送模式都是以 Partition 为分界；对于一个 Topic 的并发量限制在于有多少个 Partition, 就能支撑多少的并发；</li>
</ul>
</blockquote>
<h2 id="Kafka-快速入门">Kafka 快速入门</h2>
<h3 id="安装部署">安装部署</h3>
<h4 id="集群规划">集群规划</h4>
<table>
<thead>
<tr>
<th>centos_02(192.168.179.131)</th>
<th>centos_03(192.168.179.132)</th>
<th>centos_04(192.168.179.133)</th>
</tr>
</thead>
<tbody>
<tr>
<td>zk</td>
<td>zk</td>
<td>zk</td>
</tr>
<tr>
<td>kafka</td>
<td>kafka</td>
<td>kafka</td>
</tr>
</tbody>
</table>
<h4 id="集群部署">集群部署</h4>
<blockquote>
<p>注：<strong>搭建Kafka集群时，需要先搭建zookeeper集群</strong></p>
<p>可以参考别人的教程：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/gubeichengxuyuan/article/details/125064114">https://blog.csdn.net/gubeichengxuyuan/article/details/125064114</a></p>
</blockquote>
<ol>
<li>官方下载地址：<a target="_blank" rel="noopener" href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a></li>
</ol>
<p>下载<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/kafka/3.0.0/kafka_2.12-3.0.0.tgz">kafka_2.12-3.0.0.tgz</a></p>
<ol start="2">
<li>解压安装包</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka_2.12-3.0.0.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>在<code>/opt/module/</code>目录下修改解压后的文件名</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv kafka_2.12-3.0.0/ kafka</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>进入<code>/opt/module/kafka</code>目录，修改配置文件</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd config/</span><br><span class="line">vim server.properties</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="section"># broker的全局唯一编号，不能重复，只能是数字</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="section"># 处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="section"># 用来处理磁盘IO的线程数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="section"># 发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="section"># 接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="section"># 请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="section"># Kafka运行日志（数据）存放的路径，路径不需要提前创建，Kafka自动帮你创建，可以配置多个磁盘路径，路径与路径之间可以用&quot;, &quot;分隔</span></span><br><span class="line">log.dirs=/tmp/kafka-logs</span><br><span class="line"><span class="section"># topic在当前broker上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="section"># 用来恢复和清理data下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="section"># 每个topic创建时的副本数，默认是1个副本</span></span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line"><span class="section"># segment 文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="section"># 每个segment文件的大小，默认最大1G</span></span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"><span class="section"># 检查过期数据的时间，默认5分钟检查一次是否数据过期</span></span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"><span class="section"># 配置连接zookeeper集群地址（在zk根目录下创建/kafka,方便管理）</span></span><br><span class="line">zookeeper.connect=localhost:2181</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>修改以下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">broker的全局唯一编号，不能重复，只能是数字</span></span><br><span class="line">broker.id=0</span><br><span class="line"></span><br><span class="line">log.dirs=/opt/module/kafka/datas</span><br><span class="line"></span><br><span class="line">zookeeper.connect=centos_02:2181,centos_03:2181,centos_04:2181/kafka</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：如果启动kafka遇到问题，可以尝试将hostname改成IP地址</p>
</blockquote>
<ol start="5">
<li>三台虚拟机都按照上述配置文件修改对应的文件</li>
</ol>
<blockquote>
<p>注：<a target="_blank" rel="noopener" href="http://broker.id">broker.id</a> 不得重复，整个集群中唯一：</p>
</blockquote>
<ol start="6">
<li>配置环境变量</li>
</ol>
<p>6.1 在<code>/etc/profile.d/my_env.sh</code> 文件中增加kafka环境变量配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>
<p>6.2 增加内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure>
<p>6.3 刷新以下环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<ol start="7">
<li>启动集群</li>
</ol>
<p>7.1 必须先启动zookeeper集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost kafka]# zookeeper-server-start.sh -daemon config/zookeeper.properties</span><br></pre></td></tr></table></figure>
<p>检查zookeeper是否启动成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ps -ef | grep zookeeper</span><br></pre></td></tr></table></figure>
<p>7.2 依次在三台虚拟机节点上启动Kafka</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure>
<p>检查kafka是否启动成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>
<h4 id="利用脚本管理kafka的启动和停止">利用脚本管理kafka的启动和停止</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">    for i in centos_02 centos_03 centos_04</span><br><span class="line">    do</span><br><span class="line">        echo &quot;---- start $i kafka ----&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties&quot;</span><br><span class="line">    done</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">    for i in centos_02 centos_03 centos_04</span><br><span class="line">    do</span><br><span class="line">        echo &quot;---- stop $i kafka ----&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka/bin/kafka-server-stop.sh&quot;</span><br><span class="line">    done</span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<h3 id="Kafka-命令行操作">Kafka 命令行操作</h3>
<h4 id="主题命令行操作">主题命令行操作</h4>
<ol>
<li>查看操作主题命令参数</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941400.png" alt="image-20221004155723600"></p>
<ol start="2">
<li>查看当前服务器中的所有topic</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --bootstrap-server 192.168.179.131:9092 --list</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>创建 first topic</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --bootstrap-server 192.168.179.131:9092 --create --partitions 1 --replication-factor 3 --topic first</span><br></pre></td></tr></table></figure>
<p>选项说明：</p>
<ul>
<li>–topic：定义 topic 名</li>
<li>–replication-factor：定义副本数</li>
<li>–partitions：定义分区数</li>
</ul>
<ol start="4">
<li>查看 first 主题的详情</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --bootstrap-server 192.168.179.131:9092 --topic first --describe</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>修改分区数（注意：分区数只能增加，不能减少）</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --bootstrap-server 192.168.179.131:9092 --alter --topic first --partitions 3</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>删除 topic</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --bootstrap-server 192.168.179.131:9092 --topic first --delete</span><br></pre></td></tr></table></figure>
<h4 id="生产者命令行操作">生产者命令行操作</h4>
<ol>
<li>查看操作生产者命令参数</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941401.png" alt="image-20221004170935402"></p>
<ol start="2">
<li>发送消息</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --bootstrap-server 192.168.179.131:9092 --topic first</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello world</span></span><br></pre></td></tr></table></figure>
<h4 id="消费者命令行操作">消费者命令行操作</h4>
<ol>
<li>查看操作消费者命令参数</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941402.png" alt="image-20221004171320180"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941403.png" alt="image-20221004171332188"></p>
<ol start="2">
<li>消费消息</li>
</ol>
<p>2.1 消费 first 主题中的数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server 192.168.179.131:9092 --topic first</span><br></pre></td></tr></table></figure>
<h2 id="Kafka-生产者">Kafka 生产者</h2>
<h3 id="生产者消息发送流程">生产者消息发送流程</h3>
<h4 id="发送原理">发送原理</h4>
<p>在消息发送的过程中，涉及到了<strong>两个线程——main 线程和Sender 线程</strong>。在main 线程中创建了<strong>一个双端队列RecordAccumulator</strong>。main 线程将消息发送给RecordAccumulator，Sender 线程不断从RecordAccumulator 中拉取消息发送到Kafka Broker。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941404.png" alt="image-20221004200520496"></p>
<h4 id="生产者重要参数列表">生产者重要参数列表</h4>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941405.png" alt="image-20221004201223154"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941406.png" alt="image-20221004201238016"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941407.png" alt="image-20221004201257108"></p>
<h3 id="异步发送-API">异步发送 API</h3>
<h4 id="普通异步发送">普通异步发送</h4>
<ol>
<li>需求：创建Kafka生产者，采用异步的方式发送到Kafka Broker</li>
</ol>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941408.png" alt="image-20221006101544168"></p>
<ol start="2">
<li>代码编写</li>
</ol>
<p>1）创建工程Kafka</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>3）创建包名：<code>com.atguigu.kafka.producer</code></p>
<p>4）编写不带回调函数的API代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 连接集群</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092，192.168.179.132:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定对应的 key 和 value （必须序列化）key.serializer value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建Kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu&quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试：</p>
<p>1）在centos_03上开启kafka消费者</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost kafka]# bin/kafka-console-consumer.sh --bootstrap-server 192.168.179.131:9092 --topic first</span><br></pre></td></tr></table></figure>
<p>2）执行上述生产者的代码，观察消费者是否接收到消息</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941409.png" alt="image-20221006132757082"></p>
<h4 id="带回调函数的异步发送">带回调函数的异步发送</h4>
<p>回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，粉笔是元数据信息（RecordMetadata）和异常信息（Exception），如果 Exception 为 null, 说明消息发送成功，如果 Exception 不为 null，说明消息发送失败。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941410.png" alt="image-20221006133347796"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallback</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 连接集群</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092，192.168.179.132:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定对应的 key 和 value （必须序列化）key.serializer value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建Kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题: &quot;</span> + metadata.topic() + <span class="string">&quot;, 分区: &quot;</span> + metadata.partition());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="同步发送API">同步发送API</h3>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941411.png" alt="image-20221006133658777"></p>
<p><strong>只需在异步发送的基础上，在调用一下get()方法即可。</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerSync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, ExecutionException &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 连接集群</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092，192.168.179.132:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定对应的 key 和 value （必须序列化）key.serializer value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建Kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu&quot;</span> + i)).get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="生产者分区">生产者分区</h3>
<h4 id="分区好处">分区好处</h4>
<ol>
<li><strong>便于合理使用存储资源</strong>。每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现<strong>负载均衡</strong>的效果。</li>
<li><strong>提高并行度</strong>。生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941413.png" alt="image-20221006134154899"></p>
<h4 id="生产者发送消息的分区策略">生产者发送消息的分区策略</h4>
<ol>
<li><strong>默认的分区器 DefaultPartitioner</strong></li>
</ol>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941414.png" alt="image-20221006135839620"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941415.png" alt="image-20221006135856390"></p>
<h4 id="自定义分区器">自定义分区器</h4>
<ol>
<li>
<p>需求：例如我们实现一个分区器，实现发送过来的数据中如果包含 atguigu，就发往0号分区，不包含 atguigu，就发往1号分区。</p>
</li>
<li>
<p>实现步骤</p>
<ol>
<li>定义类实现 Partitioner 接口</li>
<li>重写 partition() 方法</li>
</ol>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyPartitioner</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String topic, Object key, <span class="type">byte</span>[] keyBytes, Object value, <span class="type">byte</span>[] valueBytes, Cluster cluster)</span> &#123;</span><br><span class="line">        <span class="comment">// 获取数据 atguigu hello</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">msgValues</span> <span class="operator">=</span> value.toString();</span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line">        <span class="keyword">if</span> (msgValues.contains(<span class="string">&quot;atguigu&quot;</span>)) &#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>使用分区器的方法，在生产者的配置中添加分区器参数</li>
</ol>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941416.png" alt="image-20221006141529678"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallbackPartition</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 连接集群</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092，192.168.179.132:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定对应的 key 和 value （必须序列化）key.serializer value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关联自定义分区器</span></span><br><span class="line">        properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, <span class="string">&quot;com.atguigu.kafka.producer.MyPartitioner&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建Kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题: &quot;</span> + metadata.topic() + <span class="string">&quot;, 分区: &quot;</span> + metadata.partition());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="生产经验——生产者如何提高吞吐量">生产经验——生产者如何提高吞吐量</h3>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941417.png" alt="image-20221006141806031"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941418.png" alt="image-20221006152757611"></p>
<blockquote>
<p>因此，若要提高吞吐量：</p>
<ul>
<li>提高批次大小</li>
<li>提高等待时间</li>
<li>利用压缩</li>
<li>提高缓冲区大小</li>
</ul>
</blockquote>
<h3 id="生产经验——数据可靠性">生产经验——数据可靠性</h3>
<p>0）回顾发送流程</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941419.png" alt="image-20221006152831884"></p>
<p>1）ack应答原理</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941420.png" alt="image-20221006152905298"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941421.png" alt="image-20221006152952245"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941422.png" alt="image-20221006153117786"></p>
<p><strong>在代码中配置acks：</strong></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941423.png" alt="image-20221006154022249"></p>
<h3 id="生产经验——数据去重">生产经验——数据去重</h3>
<p>要想解决生产者的数据重复问题，需要引入<strong>幂等性</strong>和<strong>事务</strong></p>
<h4 id="数据传递语义">数据传递语义</h4>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941424.png" alt="image-20221006154132145"></p>
<h4 id="幂等性">幂等性</h4>
<p>默认打开</p>
<ol>
<li><strong>幂等性原理</strong></li>
</ol>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941425.png" alt="image-20221006154319788"></p>
<ol start="2">
<li><strong>如何使用幂等性</strong></li>
</ol>
<p>开启参数<code>enable.idempotence</code> 默认为true，false 关闭。</p>
<h4 id="生产者事务">生产者事务</h4>
<ol>
<li>Kafka 事务原理</li>
</ol>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941426.png" alt="image-20221006154633410"></p>
<ol start="2">
<li>Kafka的事务一共有如下5个API</li>
</ol>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941427.png" alt="image-20221006155101921"></p>
<ol start="3">
<li>代码演示（单个Producer，使用事务保证消息的仅一次发送）</li>
</ol>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941428.png" alt="image-20221006155805664"></p>
<h3 id="生产经验——数据有序">生产经验——数据有序</h3>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941429.png" alt="image-20221007092629321"></p>
<h3 id="生产经验——数据乱序">生产经验——数据乱序</h3>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941430.png" alt="image-20221007093052147"></p>
<h2 id="Kafka-Broker">Kafka Broker</h2>
<h3 id="Kafka-Broker-工作流程">Kafka Broker 工作流程</h3>
<h4 id="Zookeeper-存储的Kafka信息">Zookeeper 存储的Kafka信息</h4>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941431.png" alt="image-20221007094324179"></p>
<p>如下图可以看到，Zookeeper的服务端存储的Kafka相关信息有：</p>
<ul>
<li><code>/kafka/brokers/ids</code>：[0,1,2]；	主要记录了当前Kafka有几个服务器作为broker，其中会包含服务器的hostname等信息。</li>
<li><code>/kafka/brokers/topics/first/partitions/0/state</code>：记录了哪个服务器是Leader，以及目前可用的服务器有哪些（ISR）。</li>
<li><code>/kafka/controller</code>：用于辅助选举Leader。（注：每个broker节点都有一个controller）</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941432.png" alt="image-20230325154207565"></p>
<blockquote>
<p>注：</p>
<ul>
<li><code>isr</code> 表示：<code>leader</code>和<code>follower</code>之间，通讯正常的节点。</li>
</ul>
</blockquote>
<h4 id="Kafka-Broker-总体工作流程">Kafka Broker 总体工作流程</h4>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941433.png" alt="image-20221007094932531"></p>
<ol>
<li><code>broker</code>启动后，在zk中注册</li>
<li>哪个<code>broker</code>节点的<code>controller</code>抢先在zk中注册了，其余<code>broker</code>节点的<code>controller</code>就不会在zk中出现。</li>
<li>选举出来的<code>controller</code>会先监听zk中``/kafka/brokers`节点变化</li>
<li>选举出来的<code>controller</code>决定<code>Leader</code>的选举。选举规则如下：</li>
</ol>
<ul>
<li><strong>在<code>isr</code>中存活为前提</strong>。</li>
<li>按照<code>AR</code>中排在前面的节点会被优先选举为<code>Leader</code>。</li>
</ul>
<p>（例如，<code>ar</code>=[1,0,2]，<code>isr</code>=[1,0,2]，那么leader选举顺序会按照1，0，2的顺序轮询）</p>
<ol start="5">
<li><code>controller</code>会将选举出来的节点信息上传到zk。</li>
<li>其他<code>broker</code>的<code>controller</code>从zk同步相关信息。</li>
<li>当生产者向<code>kafka</code>发送信息时，<code>Leader</code>先接收到信息，然后<code>Follower</code>主动跟<code>Leader</code>同步信息，然后给生产者应答消息。</li>
<li>如果<code>Leader</code>挂了，那<code>controller</code>能监听到节点变化</li>
<li><code>controller</code>再次获取zk中存储的<code>isr</code>信息</li>
<li>选举出新的<code>Leader</code></li>
<li>更新<code>Leader</code>及<code>isr</code></li>
</ol>
<blockquote>
<p>注：</p>
<ul>
<li><code>AR</code>表示：Kafka分区中所有的副本，包括挂掉的节点。</li>
</ul>
</blockquote>
<hr/>
<p><strong>模拟<code>Kafka</code>下线，观察<code>Zookeeper</code>中数据变化</strong></p>
<ol>
<li>先查看<code>/kafka/brokers/ids</code>路径上的节点。</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">连接到集群中任意zookeeper节点服务器</span></span><br><span class="line">[root@centos_03 kafka]# zkCli.sh -server 192.168.179.131:2181</span><br><span class="line"></span><br><span class="line">[zk: 192.168.179.131:2181(CONNECTED) 0] ls /kafka/brokers/ids</span><br><span class="line"></span><br><span class="line">[0, 1, 2]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>查看<code>/kafka/controller</code>路径上的数据</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: 192.168.179.131:2181(CONNECTED) 1] get /kafka/controller</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;brokerid&quot;:2,&quot;timestamp&quot;:&quot;1679728326055&quot;&#125;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>查看<code>/kafka/brokers/topics/first/partitions/0/state</code>上的数据</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: 192.168.179.131:2181(CONNECTED) 2] get /kafka/brokers/topics/first/partitions/0/state</span><br><span class="line"></span><br><span class="line">&#123;&quot;controller_epoch&quot;:2,&quot;leader&quot;:2,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[2,1,0]&#125;</span><br></pre></td></tr></table></figure>
<p>可以观察到上述说明<code>Leader</code>节点的<code>broker.id</code>是2</p>
<ol start="4">
<li>停止centos_04上的kafka</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_04 kafka]# bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>再次查看<code>/kafka/brokers/ids</code>路径上的节点。</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: 192.168.179.131:2181(CONNECTED) 3] ls /kafka/brokers/ids</span><br><span class="line"></span><br><span class="line">[0, 1]</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>再次查看<code>/kafka/controller</code>路径上的数据</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: 192.168.179.131:2181(CONNECTED) 4] get /kafka/controller</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;brokerid&quot;:1,&quot;timestamp&quot;:&quot;1679733548478&quot;&#125;</span><br></pre></td></tr></table></figure>
<ol start="7">
<li>再次查看<code>/kafka/brokers/topics/first/partitions/0/state</code>上的数据</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: 192.168.179.131:2181(CONNECTED) 5] get /kafka/brokers/topics/first/partitions/0/state</span><br><span class="line"></span><br><span class="line">&#123;&quot;controller_epoch&quot;:2,&quot;leader&quot;:1,&quot;version&quot;:1,&quot;leader_epoch&quot;:1,&quot;isr&quot;:[1,0]&#125;</span><br></pre></td></tr></table></figure>
<p>可以观察到上述说明<code>Leader</code>节点的<code>broker.id</code>是1</p>
<ol start="8">
<li>启动centos_04上的kafka</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_04 kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure>
<ol start="9">
<li>再次观察1、2、3步骤中的内容</li>
</ol>
<h3 id="生产经验——节点服役和退役">生产经验——节点服役和退役</h3>
<h4 id="服役新节点">服役新节点</h4>
<p>有一个正在运行中的<code>kafka</code>集群，如果需要新增一个节点，那应该如何做？</p>
<hr/>
<p><strong>1）新节点的准备</strong></p>
<ul>
<li>
<p>关闭centos_04，并右键执行克隆操作。克隆出一个centos_05</p>
</li>
<li>
<p>在VMware中修改centos_05的MAC地址</p>
</li>
<li>
<p>开启centos_05，并修改IP地址、UUID、hostname等。</p>
</li>
<li>
<p>重新启动centos_04和centos_05</p>
</li>
</ul>
<hr/>
<ul>
<li>
<p>修改centos_05中的kafka中的配置文件<code>server.properties</code>的<code>broker.id</code>为3.（注：只修改<code>broker.id</code>即可）</p>
</li>
<li>
<p>删除centos_05中残余的<code>datas</code>和<code>logs</code>目录</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_05 kafka]# rm -rf datas/ logs/</span><br></pre></td></tr></table></figure>
<ul>
<li>启动centos_02、centos_03、centos_04中的zookeeper集群与kafka集群</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zkServer.sh start</span><br><span class="line"></span><br><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure>
<ul>
<li>单独启动centos_05中的kafka</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure>
<p><strong>查看之前创建的topic主题情况</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --topic first --describe</span><br><span class="line"></span><br><span class="line">Topic: first    TopicId: LzWgw9oSSz682IOfozjtWg PartitionCount: 1       ReplicationFactor: 3    Configs: segment.bytes=1073741824</span><br><span class="line">        Topic: first    Partition: 0    Leader: 2       Replicas: 2,1,0 Isr: 0,2,1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从中可以看到，副本依旧在[0, 1, 2]节点中，并没有在节点3中。</p>
<hr/>
<p><strong>2）执行负载均衡操作</strong></p>
<p>为了让节点3可以分担历史数据的存储，需要执行负载均衡操作。</p>
<ul>
<li>创建一个要均衡的主题</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# vim topics-to-move.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">        &quot;topics&quot;: [</span><br><span class="line">                &#123;&quot;topic&quot;: &quot;first&quot;&#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;version&quot;: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>生成一个负载均衡的计划</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server centos_02:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,1,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure>
<p>如果不满足要求可以重新生成</p>
<ul>
<li>创建副本存储计划（所有副本存储在<code>broker0</code>、<code>broker1</code>、<code>broker2</code>、<code>broker3</code>中）</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# vim increase-replication-factor.json</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将上述生成的计划复制下来</span></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>执行副本存储计划</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server centos_02:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,1,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignment for first-0</span><br></pre></td></tr></table></figure>
<ul>
<li>验证副本存储计划是否执行成功</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server centos_02:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line"></span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition first-0 is complete.</span><br><span class="line"></span><br><span class="line">Clearing broker-level throttles on brokers 0,1,2,3</span><br><span class="line">Clearing topic-level throttles on topic first</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="退役旧节点">退役旧节点</h4>
<p>如果有个正在运行的kafka节点，需要退役，停止运行，那应该如何操作？如何将该节点的所有数据转移到其他节点？</p>
<hr/>
<p><strong>1）执行负载均衡操作</strong></p>
<p>先按照退役一台节点，生成执行计划，然后按照服役时操作流程执行负载均衡。</p>
<ul>
<li>创建一个要均衡的主题</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# vim topics-to-move.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">        &quot;topics&quot;: [</span><br><span class="line">                &#123;&quot;topic&quot;: &quot;first&quot;&#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;version&quot;: 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>创建执行计划</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server centos_02:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2&quot; --generate</span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1,0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在上述中，一开始<code>first</code>主题的分区0，在[3, 0, 1]上有副本。假设需要退役节点3。在执行计划后，该主题只在[0, 1, 2]节点上有副本。</p>
<ul>
<li>创建副本存储计划</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# vim increase-replication-factor.json</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1,0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>执行副本存储计划</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server centos_02:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignment for first-0</span><br></pre></td></tr></table></figure>
<ul>
<li>验证副本存储计划</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server centos_02:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line"></span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition first-0 is complete.</span><br><span class="line"></span><br><span class="line">Clearing broker-level throttles on brokers 0,1,2,3</span><br><span class="line">Clearing topic-level throttles on topic first</span><br></pre></td></tr></table></figure>
<p>2）执行停止命令（将退役节点上的kafka停止）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_05 kafka]# bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>
<h3 id="Kafka-副本">Kafka 副本</h3>
<h4 id="副本基本信息">副本基本信息</h4>
<ol>
<li><strong>Kafka 副本作用：提高数据可靠性</strong></li>
<li>Kafka 默认副本1个，生产环境一般配置为2个，保证数据可靠性；太多副本会增加磁盘存储空间，增加网络上数据传输，降低效率。</li>
<li>Kafka中副本分为：Leader 和 Follower。<strong>Kafka 生产者只会把数据发往 Leader</strong>，然后 <strong>Follower 找 Leader 进行同步数据</strong>。</li>
<li>Kafka分区中的所有副本统称为<code>AR</code>（Assigner Replicas）</li>
</ol>
<p>AR = ISR + OSR</p>
<ul>
<li>
<p><code>ISR</code>：表示和<code>Leader</code>保持同步的<code>Follower</code>集合（也包括<code>Leader</code>）。如果<code>Follower</code>长时间未向<code>Leader</code>发送通信请求或同步数据，则该<code>Follower</code>将被踢出<code>ISR</code>。该时间阈值由<code>replica.lag.time.max.ms</code>参数设定，默认30s。Leader 发生故障之后，就会从ISR 中选举新的Leader。</p>
</li>
<li>
<p><code>OSR</code>：表示Follower 与Leader 副本同步时，延迟过多的副本。</p>
</li>
</ul>
<h4 id="Leader选举流程">Leader选举流程</h4>
<p>Kafka 集群中有一个<code>broker </code>的<code>Controller </code>会被选举为<code>Controller Leader</code>，<strong>负责管理集群<code>broker </code>的上下线</strong>，所有topic 的<strong>分区副本分配</strong>和**<code>Leader </code>选举**等工作。</p>
<p>Controller 的信息同步工作是<strong>依赖于Zookeeper 的</strong>。</p>
<p><strong>Leader选举流程</strong></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941434.png" alt="image-20230325205503878"></p>
<ol>
<li><code>broker</code>启动后，在zk中注册</li>
<li>哪个<code>broker</code>节点的<code>controller</code>抢先在zk中注册了，其余<code>broker</code>节点的<code>controller</code>就不会在zk中出现。</li>
<li>选举出来的<code>controller</code>会先监听zk中``/kafka/brokers`节点变化</li>
<li>选举出来的<code>controller</code>决定<code>Leader</code>的选举。选举规则如下：</li>
</ol>
<ul>
<li><strong>在<code>isr</code>中存活为前提</strong>。</li>
<li>按照<code>AR</code>中排在前面的节点会被优先选举为<code>Leader</code>。（<strong>是按照<code>AR</code>中的顺序优先选举！</strong>）</li>
</ul>
<p>（例如，<code>ar</code>=[1,0,2]，<code>isr</code>=[1,0,2]，那么leader选举顺序会按照1，0，2的顺序轮询）</p>
<ol start="5">
<li><code>controller</code>会将选举出来的节点信息上传到zk。</li>
<li>其他<code>broker</code>的<code>controller</code>从zk同步相关信息。</li>
<li>假设<code>broker1</code>中的<code>Leader</code>挂了</li>
<li>那<code>controller</code>能监听到节点变化</li>
<li><code>controller</code>再次获取zk中存储的<code>isr</code>信息</li>
<li>选举出新的<code>Leader</code></li>
<li>更新<code>Leader</code>及<code>isr</code></li>
</ol>
<hr/>
<p><strong>验证<code>Leader</code>的选举流程：</strong></p>
<p>需要使用centos_02、centos_03、centos_04、centos_05等四台虚拟机。其中在centos_02、centos_03、centos_04搭建了<code>zookeeper</code>集群，在centos_02、centos_03、centos_04、centos_05上搭建了<code>kafka</code>集群。</p>
<ol>
<li>创建一个新的topic，4个分区，4个副本</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --create --topic world1 --partitions 4 --replication-factor 4</span><br><span class="line"></span><br><span class="line">Created topic world1.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>查看Leader分布情况</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --describe --topic world1</span><br><span class="line"></span><br><span class="line">Topic: world1   TopicId: j4tB40K-TmKlM7Joxnoo1A PartitionCount: 4       ReplicationFactor: 4    Configs: segment.bytes=1073741824</span><br><span class="line">        Topic: world1   Partition: 0    Leader: 0       Replicas: 0,3,1,2       Isr: 0,3,1,2</span><br><span class="line">        Topic: world1   Partition: 1    Leader: 2       Replicas: 2,1,0,3       Isr: 2,1,0,3</span><br><span class="line">        Topic: world1   Partition: 2    Leader: 3       Replicas: 3,0,2,1       Isr: 3,0,2,1</span><br><span class="line">        Topic: world1   Partition: 3    Leader: 1       Replicas: 1,2,3,0       Isr: 1,2,3,0</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="3">
<li>停止掉centos_05的kafka进程，并查看Leader分区情况</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在centos_05上操作</span></span><br><span class="line">[root@centos_05 kafka]# bin/kafka-server-stop.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在centos_02上操作</span></span><br><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --describe --topic world1</span><br><span class="line"></span><br><span class="line">Topic: world1   TopicId: j4tB40K-TmKlM7Joxnoo1A PartitionCount: 4       ReplicationFactor: 4    Configs: segment.bytes=1073741824</span><br><span class="line">        Topic: world1   Partition: 0    Leader: 0       Replicas: 0,3,1,2       Isr: 0,1,2</span><br><span class="line">        Topic: world1   Partition: 1    Leader: 2       Replicas: 2,1,0,3       Isr: 2,1,0</span><br><span class="line">        Topic: world1   Partition: 2    Leader: 0       Replicas: 3,0,2,1       Isr: 0,2,1</span><br><span class="line">        Topic: world1   Partition: 3    Leader: 1       Replicas: 1,2,3,0       Isr: 1,2,0</span><br></pre></td></tr></table></figure>
<p>可以看到所有的Leader节点都不会是3。并且，原本Leader=3的分区，按照规则从<code>AR</code>（即<code>Replicas</code>）[3,0,2,1] 中，选出了下一个Leader节点为0</p>
<ol start="4">
<li>停止掉centos_04的kafka进程，并查看Leader分区情况</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在centos_04上操作</span></span><br><span class="line">[root@centos_04 kafka]# bin/kafka-server-stop.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在centos_02上操作</span></span><br><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --describe --topic world1</span><br><span class="line"></span><br><span class="line">Topic: world1   TopicId: j4tB40K-TmKlM7Joxnoo1A PartitionCount: 4       ReplicationFactor: 4    Configs: segment.bytes=1073741824</span><br><span class="line">        Topic: world1   Partition: 0    Leader: 0       Replicas: 0,3,1,2       Isr: 0,1</span><br><span class="line">        Topic: world1   Partition: 1    Leader: 1       Replicas: 2,1,0,3       Isr: 1,0</span><br><span class="line">        Topic: world1   Partition: 2    Leader: 0       Replicas: 3,0,2,1       Isr: 0,1</span><br><span class="line">        Topic: world1   Partition: 3    Leader: 1       Replicas: 1,2,3,0       Isr: 1,0</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看到所有的Leader节点都不会是2或3。并且，原本Leader=2的分区，按照规则从<code>AR</code>（即<code>Replicas</code>）[2,1,0,3] 中，选出了下一个Leader节点为1</p>
<ol start="5">
<li>启动centos_05的kafka进程，并查看Leader分区情况</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_05 kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line"></span><br><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --describe --topic world1</span><br><span class="line"></span><br><span class="line">Topic: world1   TopicId: j4tB40K-TmKlM7Joxnoo1A PartitionCount: 4       ReplicationFactor: 4    Configs: segment.bytes=1073741824</span><br><span class="line">        Topic: world1   Partition: 0    Leader: 0       Replicas: 0,3,1,2       Isr: 0,1,3</span><br><span class="line">        Topic: world1   Partition: 1    Leader: 1       Replicas: 2,1,0,3       Isr: 1,0,3</span><br><span class="line">        Topic: world1   Partition: 2    Leader: 0       Replicas: 3,0,2,1       Isr: 0,1,3</span><br><span class="line">        Topic: world1   Partition: 3    Leader: 1       Replicas: 1,2,3,0       Isr: 1,0,3</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从中可以看到，虽然Leader节点没有变化，但是<code>Isr</code>节点有变化，新增了节点3.</p>
<ol start="6">
<li>启动centos_04的kafka进程，并查看Leader分区情况</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_04 kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line"></span><br><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --describe --topic world1</span><br><span class="line"></span><br><span class="line">Topic: world1   TopicId: j4tB40K-TmKlM7Joxnoo1A PartitionCount: 4       ReplicationFactor: 4    Configs: segment.bytes=1073741824</span><br><span class="line">        Topic: world1   Partition: 0    Leader: 0       Replicas: 0,3,1,2       Isr: 0,1,3,2</span><br><span class="line">        Topic: world1   Partition: 1    Leader: 1       Replicas: 2,1,0,3       Isr: 1,0,3,2</span><br><span class="line">        Topic: world1   Partition: 2    Leader: 0       Replicas: 3,0,2,1       Isr: 0,1,3,2</span><br><span class="line">        Topic: world1   Partition: 3    Leader: 1       Replicas: 1,2,3,0       Isr: 1,0,3,2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从中可以看到，虽然Leader节点没有变化，但是<code>Isr</code>节点有变化，新增了节点2.</p>
<h4 id="Leader和Follower故障处理细节">Leader和Follower故障处理细节</h4>
<p>如果Leader或者Follower挂了，底层是如何处理的？</p>
<p><strong>1. Follower故障处理细节</strong></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941435.png" alt="image-20230325212302287"></p>
<blockquote>
<p>建议看视频理解。</p>
<p>后续可能会专门再写篇博客理解</p>
</blockquote>
<p><strong>2. Leader故障处理细节</strong></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941436.png" alt="image-20230325213224956"></p>
<h4 id="分区副本分配">分区副本分配</h4>
<p>如果kafka服务器只有4个节点，那么设置kafka的分区数大于服务器台数，在kafka底层如何分配存储副本呢？</p>
<p>1）创建16个分区，3个副本</p>
<p>（1）创建一个新的topic，名称为second</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --create --partitions 16 --replication-factor 3 --topic second</span><br><span class="line"></span><br><span class="line">Created topic second.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（2）查看分区和副本情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --describe --topic second</span><br><span class="line"></span><br><span class="line">Topic: second   TopicId: NplREOVsQnKdkDw_mI4sOA PartitionCount: 16      ReplicationFactor: 3    Configs: se</span><br><span class="line">        Topic: second   Partition: 0    Leader: 1       Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">        Topic: second   Partition: 1    Leader: 0       Replicas: 0,2,3 Isr: 0,2,3</span><br><span class="line">        Topic: second   Partition: 2    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1</span><br><span class="line">        Topic: second   Partition: 3    Leader: 3       Replicas: 3,1,0 Isr: 3,1,0</span><br><span class="line">        Topic: second   Partition: 4    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3</span><br><span class="line">        Topic: second   Partition: 5    Leader: 0       Replicas: 0,3,1 Isr: 0,3,1</span><br><span class="line">        Topic: second   Partition: 6    Leader: 2       Replicas: 2,1,0 Isr: 2,1,0</span><br><span class="line">        Topic: second   Partition: 7    Leader: 3       Replicas: 3,0,2 Isr: 3,0,2</span><br><span class="line">        Topic: second   Partition: 8    Leader: 1       Replicas: 1,3,0 Isr: 1,3,0</span><br><span class="line">        Topic: second   Partition: 9    Leader: 0       Replicas: 0,1,2 Isr: 0,1,2</span><br><span class="line">        Topic: second   Partition: 10   Leader: 2       Replicas: 2,0,3 Isr: 2,0,3</span><br><span class="line">        Topic: second   Partition: 11   Leader: 3       Replicas: 3,2,1 Isr: 3,2,1</span><br><span class="line">        Topic: second   Partition: 12   Leader: 1       Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">        Topic: second   Partition: 13   Leader: 0       Replicas: 0,2,3 Isr: 0,2,3</span><br><span class="line">        Topic: second   Partition: 14   Leader: 2       Replicas: 2,3,1 Isr: 2,3,1</span><br><span class="line">        Topic: second   Partition: 15   Leader: 3       Replicas: 3,1,0 Isr: 3,1,0</span><br></pre></td></tr></table></figure>
<h4 id="生产经验——手动调整分区副本存储">生产经验——手动调整分区副本存储</h4>
<p>​	在生产环境中，每台服务器的配置和性能不一致，但是Kafka只会根据自己的代码规则创建对应的分区副本，就会导致个别服务器存储压力较大。所有需要手动调整分区副本的存储。</p>
<blockquote>
<p>需求：创建一个新的topic，4个分区，两个副本，名称为three。将该topic的所有副本都存储到broker0和broker1两台服务器上。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941437.png" alt="image-20230328144813658"></p>
</blockquote>
<p><strong>手动调整分区副本存储的步骤如下：</strong></p>
<p>（1）创建一个新的topic，名称为three</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --create --partitions 4 --replication-factor 2 --topic three</span><br><span class="line"></span><br><span class="line">Created topic three.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（2）查看分区副本资源情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --describe --topic three</span><br><span class="line"></span><br><span class="line">Topic: three    TopicId: Sg0i0KbqTciegwp8KhcVZw PartitionCount: 4       ReplicationFactor: 2    Configs: segment.bytes=1073741824</span><br><span class="line">        Topic: three    Partition: 0    Leader: 3       Replicas: 3,1   Isr: 3,1</span><br><span class="line">        Topic: three    Partition: 1    Leader: 1       Replicas: 1,0   Isr: 1,0</span><br><span class="line">        Topic: three    Partition: 2    Leader: 0       Replicas: 0,2   Isr: 0,2</span><br><span class="line">        Topic: three    Partition: 3    Leader: 2       Replicas: 2,3   Isr: 2,3</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（3）创建副本存储计划（所有副本都指定存储在节点0和1中）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# vim increase-replication-factor.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">	&quot;version&quot;:1,</span><br><span class="line">	&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1]&#125;,</span><br><span class="line">				&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1]&#125;,</span><br><span class="line">				&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,0]&#125;,</span><br><span class="line">				&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[1,0]&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（4）执行副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server centos_02:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[2,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignments for three-0,three-1,three-2,three-3</span><br></pre></td></tr></table></figure>
<p>（5）验证副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server centos_02:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition three-0 is complete.</span><br><span class="line">Reassignment of partition three-1 is complete.</span><br><span class="line">Reassignment of partition three-2 is complete.</span><br><span class="line">Reassignment of partition three-3 is complete.</span><br><span class="line"></span><br><span class="line">Clearing broker-level throttles on brokers 0,1,2,3</span><br><span class="line">Clearing topic-level throttles on topic three</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（6）查看分区副本存储情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --describe --topic three</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Topic: three    TopicId: Sg0i0KbqTciegwp8KhcVZw PartitionCount: 4       ReplicationFactor: 2    Configs: segment.bytes=1073741824</span><br><span class="line">        Topic: three    Partition: 0    Leader: 0       Replicas: 0,1   Isr: 1,0</span><br><span class="line">        Topic: three    Partition: 1    Leader: 1       Replicas: 0,1   Isr: 1,0</span><br><span class="line">        Topic: three    Partition: 2    Leader: 0       Replicas: 1,0   Isr: 0,1</span><br><span class="line">        Topic: three    Partition: 3    Leader: 1       Replicas: 1,0   Isr: 1,0</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="生产经验——Leader-Partition负载平衡">生产经验——Leader Partition负载平衡</h4>
<p>​		正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941438.png" alt="image-20230328151350720"></p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>auto.leader.rebalance.enable</td>
<td>默认是true。 自动Leader Partition 平衡。生产环境中，leader 重选举的代价比较大，可能会带来性能影响，建议设置为false 关闭。</td>
</tr>
<tr>
<td>leader.imbalance.per.broker.percentage</td>
<td>默认是10%。每个broker 允许的不平衡的leader的比率。如果每个broker 超过了这个值，控制器会触发leader 的平衡。</td>
</tr>
<tr>
<td>leader.imbalance.check.interval.seconds</td>
<td>默认值300 秒。检查leader 负载是否平衡的间隔时间。</td>
</tr>
</tbody>
</table>
<h4 id="生产经验——增加副本因子">生产经验——增加副本因子</h4>
<p>在生产环境当中，由于某个主题的重要等级需要提升，我们考虑增加副本。副本数的增加需要先制定计划，然后根据计划执行。</p>
<p>1）创建topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --create --partitions 3 --replication-factor 1 --topic four</span><br><span class="line"></span><br><span class="line">Created topic four.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --describe --topic four      Topic: four     TopicId: v2BCz9JZR5SQ12eId6CaLg PartitionCount: 3       ReplicationFactor: 1    Configs: segment.bytes=1073741824</span><br><span class="line">        Topic: four     Partition: 0    Leader: 2       Replicas: 2     Isr: 2</span><br><span class="line">        Topic: four     Partition: 1    Leader: 3       Replicas: 3     Isr: 3</span><br><span class="line">        Topic: four     Partition: 2    Leader: 1       Replicas: 1     Isr: 1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>2）手动增加副本存储</p>
<p>（1）创建副本存储计划（所有副本都指定存储在节点0、1、2中）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# vim increase-replication-factor.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">        &quot;version&quot;:1,</span><br><span class="line">        &quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1,2]&#125;,</span><br><span class="line">                                &#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1,2]&#125;,</span><br><span class="line">                                &#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1,2]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（2）执行副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server centos_02:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignments for four-0,four-1,four-2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="文件存储">文件存储</h3>
<h4 id="文件存储机制">文件存储机制</h4>
<p><strong>1）Topic数据的存储机制</strong></p>
<p>Topic是逻辑上的概念，而partition是物理上的概念，<strong>每个partition对应于一个log文件</strong>，该log文件中存储的就是Producer生产的数据。<strong>Producer生产的数据会被不断追加到该log文件末端</strong>，为防止log文件过大导致数据定位效率低下，<strong>Kafka采取了分片和索引机制</strong>，<strong>将每个partition分为多个segment</strong>。<strong>每个segment包括：“.index”文件、“.log”文件和.timeindex等文件</strong>。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941439.png" alt="image-20230328160620180"></p>
<p>2）思考：Topic数据到底存储在什么位置？</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941440.png" alt="image-20230328161605121"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941441.png" alt="image-20230328161621254"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941442.png" alt="image-20230328161636566"></p>
<p>3）index文件和log文件详解</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941443.png" alt="image-20230328161708188"></p>
<h4 id="文件清理策略">文件清理策略</h4>
<p>Kafa中<strong>默认的日志保存时间为7天</strong>，可以通过调整如下参数修改保存时间。</p>
<ul>
<li><code>log.retention.hours</code> ：小时，默认7天。最低优先级</li>
<li><code>log.retention.minutes</code>：分钟。</li>
<li><code>log.retention.ms</code>：最高优先级毫秒。</li>
<li><code>log.retention.check.interval.ms</code>：负责设置检查周期，默认5分钟。</li>
</ul>
<p>那么日志一旦超过了设置的时间，怎么处理呢？<br>
Kafa中提供的<strong>日志清理策略有<code>delete</code>和<code>compact</code>两种</strong>。</p>
<p>1）<code>delete</code>日志删除：将过期数据删除</p>
<ul>
<li>
<p><code>log.cleanup,policy=delete</code>： 所有数据启用删除策略</p>
<p>(1) <strong>基于时间：默认打开。以segment中所有记录中的最大时间戳作为该文件时间戳。</strong><br>
(2) 基于大小：默认关闭。超过设置的所有日志总大小，删除最早的segment.<br>
log.retention.bytes，默认等于-1，表示无穷大</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941444.png" alt="image-20230328163924348"></p>
<p>2）<code>compact</code> 日志压缩</p>
<p><code>compact</code>日志压缩：对于相同<code>key</code>的不同<code>value</code>值，只保留最后一个版本</p>
<ul>
<li><code>log.cleanup,policy=compact</code>：所有数据启用压缩策略</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941445.png" alt="image-20230328185040596"></p>
<p>压缩后的<code>offset</code>可能是不连续的，比如上图中没有6，当从这些<code>offset</code>消费消息时，将会拿到比这个<code>offset</code>大的<code>offset</code>对应的消息，实际上会拿到<code>offset</code>为7的消息，并从这个位置开始消费。</p>
<p>这种策略只适合特殊场景，比如消息的<code>key</code>是用户ID，<code>vaue</code>是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。</p>
<h3 id="Kafka如何高效读写数据">Kafka如何高效读写数据</h3>
<p>Kafka如何高效读写数据？</p>
<p>答：</p>
<p>1）<strong>Kafka本身是分布式集群，可以采用分区技术，并行度高</strong></p>
<p>2）<strong>读数据采用稀疏索引，可以快速定位要消费的数据</strong></p>
<p>3）<strong>顺序写磁盘</strong></p>
<p>​		Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s,而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p>
<p>4）<strong>页缓存+零拷贝技术</strong></p>
<ul>
<li><strong>零拷贝</strong>：Kafka的数据加工处理操作交由Kafka生产者和Kafka消费者处理。<strong>Kafka <code>Broker</code>应用层不关心存储的数据，所以就不用走应用层，传输效率高</strong>。</li>
<li><strong><code>PageCache</code>页缓存</strong> ：Kafka重度依赖底层操作系统提供的PageCache功能。当上层有写操作时，操作系统只是将数据写入PageCache。当读操作发生时，先从PageCache中查找，如果找不到，再去磁盘中读取。实际上PageCache是把尽可能多的空闲内存都当做了磁盘缓存来使用。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941446.png" alt="image-20230328190230682"></p>
<h2 id="Kafka消费者">Kafka消费者</h2>
<h3 id="Kafka消费方式">Kafka消费方式</h3>
<ul>
<li>
<p><strong>pull（拉）模式：</strong><code>consumer</code>采用从broker中主动拉取数据。<strong>Kafka采用这种方式</strong>。</p>
</li>
<li>
<p>**push（推）模式：**Kafka没有采用这种方式，因为由broker决定消息发送速率，很难适应所有消费者的消费速率。</p>
<p>如果使用push模式，则如下图所示，如果不同的消费者的消费速度不一致，那broker就无法选择合适的发送速率</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941447.png" alt="image-20230330105708446"></p>
<p><code>pull</code>模式不足之处是：<strong>如果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据</strong>。</p>
<h3 id="Kafka消费者工作流程">Kafka消费者工作流程</h3>
<h4 id="消费者总体工作流程">消费者总体工作流程</h4>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941448.png" alt="image-20230330105849074"></p>
<ul>
<li>新版本的Kafka的<code>offset</code>存储在系统主题中，基于硬盘存储的</li>
<li>老版本（0.9）之前的Kafka的<code>offset</code>存储在对应的zookeeper中。</li>
</ul>
<h4 id="消费者组原理">消费者组原理</h4>
<p>Consumer Group（CG）：消费者组，由多个consumer组成。</p>
<p>形成一个消费者组的条件是：所有消费者的<code>groupid</code>相同。</p>
<ul>
<li>消费者组内 每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费。</li>
<li>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941449.png" alt="image-20230330110844487"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941450.png" alt="image-20230330150854548"></p>
<h4 id="消费者组初始化流程">消费者组初始化流程</h4>
<p>每个<code>broker</code>都有一个<code>coordinator</code>组件。</p>
<p>组件<code>coordinator</code>的作用：辅助实现消费者组的初始化和分区的分配。</p>
<blockquote>
<p>消费者组选择哪个<code>coordinator</code>来进行后续工作？</p>
<p><code>coordinator</code>节点选择=<code>groupid</code>的hashcode值 % 50</p>
<hr/>
<p>上述 为什么是对 50 取余？</p>
<p>50是<code>__consumer_offsets</code>默认的分区数量。<code>__consumer_offsets</code>即位移主题，Kafka将consumer的位移数据作为一条条普通的Kafka消息，提交到<code>__consumer_offsets</code>中。即<code>__consumer_offsets</code>的主要作用是保存Kafka消费者的位移信息。</p>
</blockquote>
<p>例如： <code>groupid</code>的hashcode值= 1，1% 50 = 1，那么<code>__consumer_offsets</code> 主题的1号分区，在哪个broker上，就选择这个节点的<code>coordinator</code>作为这个消费者组的老大。消费者组下的所有的消费者提交offset的时候就往这个分区去提交offset。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941451.png" alt="image-20230330151421618"></p>
<p><strong>消费者组初始化流程：</strong></p>
<ol>
<li>所有的消费者都会主动向已选择的<code>coordinator</code>发送<code>JoinGroup</code>请求（根据<code>groupid</code>请求加入某个消费者组中）</li>
<li><code>coordinator</code>会从同一个消费者组中选出一个消费者作为<code>Leader</code></li>
<li><code>coordinator</code>会把要消费的<code>topic</code>情况发送给消费者<code>Leader</code></li>
<li>消费者<code>Leader</code>会负责制定消费方案（消费方案：指定每个消费者应该各自消费哪个分区）</li>
<li>消费者<code>Lader</code>把消费方案发送给<code>coordinator</code></li>
<li><code>coordinator</code>把消费方案发给各个<code>consumer</code></li>
<li>每个消费者都会和<code>coordinator</code>保持心跳（默认3s），一旦超时（<code>session.timeout.ms</code>=45s）该消费者会被移除，并触发再平衡；或者消费者处理消息的时间过长（<code>max.poll.interval.ms</code>=5min），也会认为该消费者自动下线，也会触发再平衡</li>
</ol>
<h4 id="消费者组详细消费流程">消费者组详细消费流程</h4>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941452.png" alt="image-20230330162606950"></p>
<p><strong>消费者组详细消费流程：</strong></p>
<ol>
<li>消费者首先创建一个<code>ConsumerNetworkClient</code>（消费者网络连接客户端），主要用于跟kafka集群进行交互。</li>
<li>调用<code>sendFetches</code>方法，用来发送消费请求，拉去数据。（在该方法中定义<code>Fetch.min.bytes</code>、<code>Fetch.max.wait.ms</code>、<code>Fetch.max.bytes</code>等参数）</li>
<li>调用<code>ConsumerNetworkClient</code>的<code>send</code>方法</li>
<li>通过回调方法<code>onSuccess</code>把对应的数据拉取过来。拉取过来的数据会放在一个消息队列<code>completedFetches</code>中</li>
<li>消费者一次性从消息队列中拉取500条（默认<code>Max.poll.records</code>=500）数据</li>
<li>将拉取的数据反序列化</li>
<li>将拉取的数据进行拦截器</li>
<li>处理数据</li>
</ol>
<h4 id="消费者重要参数">消费者重要参数</h4>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>bootstrap.servers</td>
<td>向Kafka 集群建立初始连接用到的host/port 列表。</td>
</tr>
<tr>
<td>key.deserializer 和value.deserializer</td>
<td>指定接收消息的key 和value 的反序列化类型。一定要写全类名。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://group.id">group.id</a></td>
<td>标记消费者所属的消费者组。</td>
</tr>
<tr>
<td>enable.auto.commit</td>
<td>默认值为true，消费者会自动周期性地向服务器提交偏移量。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://auto.commit.interval.ms">auto.commit.interval.ms</a></td>
<td>如果设置了 enable.auto.commit 的值为true， 则该值定义了消费者偏移量向Kafka 提交的频率，默认5s。</td>
</tr>
<tr>
<td>auto.offset.reset</td>
<td>当Kafka 中没有初始偏移量或当前偏移量在服务器中不存在（如，数据被删除了），该如何处理？<br/> <code>earliest</code>：自动重置偏移量到最早的偏移量。<br/> <code>latest</code>：默认，自动重置偏移量为最新的偏移量。<br/> <code>none</code>：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常。<br/> <code>anything</code>：向消费者抛异常。</td>
</tr>
<tr>
<td>offsets.topic.num.partitions</td>
<td>__consumer_offsets 的分区数，默认是50 个分区。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://heartbeat.interval.ms">heartbeat.interval.ms</a></td>
<td>Kafka 消费者和coordinator 之间的心跳时间，默认3s。该条目的值必须小于 <a target="_blank" rel="noopener" href="http://session.timeout.ms">session.timeout.ms</a> ，<a target="_blank" rel="noopener" href="http://xn--session-3w3kyxxoq96lrw3hqvsb.timeout.ms">也不应该高于session.timeout.ms</a> 的1/3。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://session.timeout.ms">session.timeout.ms</a></td>
<td>Kafka 消费者和coordinator 之间连接超时时间，默认45s。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://max.poll.interval.ms">max.poll.interval.ms</a></td>
<td>消费者处理消息的最大时长，默认是5分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>fetch.min.bytes</td>
<td>默认1个字节。消费者获取服务器端一批消息最小的字节数。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://fetch.max.wait.ms">fetch.max.wait.ms</a></td>
<td>默认500ms。如果没有从服务器端获取到一批数据的最小字节数。该时间到，仍然会返回数据。</td>
</tr>
<tr>
<td>fetch.max.bytes</td>
<td>默认Default: 52428800 (50m)。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值 (50m) 仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受message.max.bytes（broker config) or max.message.bytes (topic config) 影响。</td>
</tr>
<tr>
<td>max.poll.records</td>
<td>一次poll拉取数据返回消息的最大条数，默认是500条。</td>
</tr>
</tbody>
</table>
<h3 id="消费者API">消费者API</h3>
<h4 id="独立消费者案例（订阅主题）">独立消费者案例（订阅主题）</h4>
<p>1）需求</p>
<p>创建一个独立消费者，消费first主题中的数据</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941453.png" alt="image-20230331092637365"></p>
<p>注意：在消费者API代码中必须配置消费者组id。命令行启动消费者时，若不填写消费者组id，则会被自动填写随机的消费者组id</p>
<p>2）实现步骤</p>
<p>（1）创建包名：com.atguigu.kafka.consumer</p>
<p>（2）编写代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">// 连接 bootstrap.servers</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092,192.168.179.132:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 反序列化</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        <span class="comment">// groupid</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        <span class="comment">// 1. 创建一个消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">        <span class="comment">// 2. 定义主题 first</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line">        <span class="comment">// 3. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置每秒拉取一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 打印拉取到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3）测试</p>
<p>（1）在IDEA中执行消费者程序</p>
<p>（2）在Kafka集群控制台中，创建Kafka生产者，并输入数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-console-producer.sh --bootstrap-server centos_02:9092 --topic first</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello</span></span><br></pre></td></tr></table></figure>
<p>（3）在IDEA控制台观察到接收到的数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 15, CreateTime = 1680231501352, serialized key size = -1, serialized value size = 5, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = hello)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：如果出现“Kafka在命令行中启动消费者可以正常消费数据，但是在通过Java编写的消费者程序中无法消费数据”这种情况，那么问题可能就出在client和服务端的连接上。</p>
<p>解决方案如下：</p>
<p>将<code>kafka/config/server.properties</code>配置文件中的<code>advertised.listeners</code>改成如下属性。其中，<code>192.168.179.131</code>是Kafka服务器的IP地址。改完重启Kafka即可。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">advertised.listeners=PLAINTEXT://192.168.179.131:9092</span><br></pre></td></tr></table></figure>
<p><code>advertised.listeners</code>的意思是说：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Hostname and port the broker will advertise to producers and consumers. If not <span class="built_in">set</span>,</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">it uses the value <span class="keyword">for</span> <span class="string">&quot;listeners&quot;</span> <span class="keyword">if</span> configured.  Otherwise, it will use the value</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">returned from java.net.InetAddress.getCanonicalHostName().</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">advertised.listeners = PLAINTEXT://your.host.name:9092</span></span><br></pre></td></tr></table></figure>
<p>即，<code>Kafka</code>保证了hostname和port都会广播给生产者和消费者。如果没有配置<code>advertised.listeners</code>这个属性，则会使用<code>listeners</code>的值。如果<code>listeners</code>也没配置的话，则使用<code>java.net.InetAddress.getCanonicalHostName() </code>（这里也就是返回localhost了）</p>
</blockquote>
<h4 id="独立消费者案例（订阅分区）">独立消费者案例（订阅分区）</h4>
<p>让一个独立消费者，消费指定的分区</p>
<p>1）需求：创建一个独立消费者，消费<code>first</code>主题0号分区的数据</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941454.png" alt="image-20230331154607888"></p>
<p>2）实现步骤</p>
<p>（1）代码编写</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerPartition</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092,192.168.179.132:9092&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        <span class="comment">// 1. 创建一个消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">        <span class="comment">// 2. 订阅主题对应的分区</span></span><br><span class="line">        ArrayList&lt;TopicPartition&gt; partitions = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        partitions.add(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;first&quot;</span>, <span class="number">0</span>));</span><br><span class="line">        kafkaConsumer.assign(partitions);</span><br><span class="line">        <span class="comment">// 3. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3）测试</p>
<p>（1）在IDEA中执行消费者程序</p>
<p>（2）在IDEA中执行生产者程序</p>
<p>生产者代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallbackPartition</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">// 连接集群</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092，192.168.179.132:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 指定对应的 key 和 value （必须序列化）key.serializer value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        <span class="comment">// 关联自定义分区器</span></span><br><span class="line">        properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, <span class="string">&quot;com.atguigu.kafka.producer.MyPartitioner&quot;</span>);</span><br><span class="line">        <span class="comment">// 1. 创建Kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(properties);</span><br><span class="line">        <span class="comment">// 2. 发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="number">0</span>, <span class="string">&quot;&quot;</span>, <span class="string">&quot;atguigu&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题: &quot;</span> + metadata.topic() + <span class="string">&quot;, 分区: &quot;</span> + metadata.partition());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">       <span class="comment">// 3. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>发送数据如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">主题: first, 分区: 0</span><br><span class="line">主题: first, 分区: 0</span><br><span class="line">主题: first, 分区: 0</span><br><span class="line">主题: first, 分区: 0</span><br><span class="line">主题: first, 分区: 0</span><br><span class="line">主题: first, 分区: 0</span><br><span class="line">主题: first, 分区: 0</span><br><span class="line">主题: first, 分区: 0</span><br><span class="line">主题: first, 分区: 0</span><br><span class="line">主题: first, 分区: 0</span><br></pre></td></tr></table></figure>
<p>（3）消费者接收到的数据如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 17, CreateTime = 1680250731070, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu0)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 18, CreateTime = 1680250731081, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu1)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 19, CreateTime = 1680250731081, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu2)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 20, CreateTime = 1680250731081, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu3)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 21, CreateTime = 1680250731081, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu4)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 22, CreateTime = 1680250731081, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu5)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 23, CreateTime = 1680250731081, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu6)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 24, CreateTime = 1680250731081, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu7)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 25, CreateTime = 1680250731081, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu8)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 17, offset = 26, CreateTime = 1680250731081, serialized key size = 0, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu9)</span><br></pre></td></tr></table></figure>
<h4 id="消费者组案例">消费者组案例</h4>
<p>1）需求：测试同一个主题的分区数据，只能由同一消费者组中的 一个消费者来消费。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941455.png" alt="image-20230331163815620"></p>
<p>2）案例实操</p>
<p>（1）复制一份基础消费者的代码，在IDEA中同时启动，即可启动同一个消费者组中的三个消费者</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">public class CustomConsumer1 &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // 0. 配置</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        // 连接 bootstrap.servers</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.179.131:9092,192.168.179.132:9092&quot;);</span><br><span class="line">        // 反序列化</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        // groupid</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test&quot;);</span><br><span class="line">        // 1. 创建一个消费者</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(properties);</span><br><span class="line">        // 2. 定义主题 first</span><br><span class="line">        ArrayList&lt;String&gt; topics = new ArrayList&lt;&gt;();</span><br><span class="line">        topics.add(&quot;first&quot;);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line">        // 3. 消费数据</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            // 设置每秒拉取一批数据</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(1));</span><br><span class="line">            // 打印拉取到的数据</span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（2）启动代码中的生产者发送消息，在IDEA控制台中即可看到三个消费者在消费不同分区的数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">public class CustomProducerCallback &#123;</span><br><span class="line">    public static void main(String[] args) throws InterruptedException &#123;</span><br><span class="line">        // 0. 配置</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        // 连接集群</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.179.131:9092，192.168.179.132:9092&quot;);</span><br><span class="line">        // 指定对应的 key 和 value （必须序列化）key.serializer value.serializer</span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        // 1. 创建Kafka生产者对象</span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(properties);</span><br><span class="line">        // 2. 发送数据</span><br><span class="line">        for (int i = 0; i &lt; 500; i++) &#123;</span><br><span class="line">            kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, &quot;atguigu&quot; + i), new Callback() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void onCompletion(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line">                    if (exception == null) &#123;</span><br><span class="line">                        System.out.println(&quot;主题: &quot; + metadata.topic() + &quot;, 分区: &quot; + metadata.partition());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            Thread.sleep(1);</span><br><span class="line">        &#125;</span><br><span class="line">        // 3. 关闭资源</span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在生产者中会随机往<code>first</code>主题的3个分区中发送数据。观察三个消费者控制台接收到的数据可以看到，每个消费者只会消费一个分区的数据。</p>
<p>（3）重新发送到一个全新的主题中，由于默认创建的主题分区数为1，可以看到只能有一个消费者消费到数据。</p>
<h3 id="生产经验——分区的分配以及再平衡">生产经验——分区的分配以及再平衡</h3>
<p><strong>思考：</strong> 一个consumer group中有多个consumer组成，一个topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个partition的数据。</p>
<p><strong>答</strong>：</p>
<p><strong>Kafka有四种主流的分区分配策略</strong>：Range、RoundRobin、Sticky、CooperativeSticky，可以通过配置参数<code>partition.assignment.strategy</code>来修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略。Kafka也可以自定义分区分配策略，不过用的比较少。</p>
<hr/>
<p>回顾一下消费者组的初始化流程：</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941456.png" alt="image-20230331165622257"></p>
<p>上述步骤中，所谓的**“消费方案”**即是Kafka的分区分配策略。</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="http://heartbeat.interval.ms">heartbeat.interval.ms</a></td>
<td>Kafka 消费者和coordinator 之间的心跳时间，默认3s。该条目的值必须小于 <a target="_blank" rel="noopener" href="http://session.timeout.ms">session.timeout.ms</a> ，<a target="_blank" rel="noopener" href="http://xn--session-3w3kyxxoq96lrw3hqvsb.timeout.ms">也不应该高于session.timeout.ms</a> 的1/3。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://session.timeout.ms">session.timeout.ms</a></td>
<td>Kafka 消费者和coordinator 之间连接超时时间，默认45s。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://max.poll.interval.ms">max.poll.interval.ms</a></td>
<td>消费者处理消息的最大时长，默认是5分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>partition.assignment.strategy</td>
<td>消费者分区分配策略，默认策略是 Range + CooperativeSticky。Kafka 可以同时使用多个分区分配策略。可以选择的策略包括： Range 、RoundRobin 、Sticky 、CooperativeSticky</td>
</tr>
</tbody>
</table>
<h4 id="Range-以及再平衡">Range 以及再平衡</h4>
<p><strong>1）Range分区策略原理</strong></p>
<p><strong>Range是根据每个topic来分配分区的</strong></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941457.png" alt="image-20230331171637532"></p>
<p><strong>2）Range分区分配策略案例</strong></p>
<p>（1）修改主题 first 为7个分区</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-topics.sh --bootstrap-server centos_02:9092 --alter --topic first --partitions 7</span><br></pre></td></tr></table></figure>
<p><strong>注意：分区数可以增加，但是不能减少。</strong></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941458.png" alt="image-20230402153217378"></p>
<p>（2）复制CustomConsumer类，创建CustomConsumer2。这样可以由三个消费者CustomConsumer、CustomConsumer1、CustomConsumer2组成消费者，组名（groupid）都为“test”，同时启动3个消费者。</p>
<p>（3）启动CustomProducer生产者，发送500条消费，随机发送到不同分区。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallback</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">// 连接集群</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092，192.168.179.132:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 指定对应的 key 和 value （必须序列化）key.serializer value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        <span class="comment">// 1. 创建Kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(properties);</span><br><span class="line">        <span class="comment">// 2. 发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">500</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题: &quot;</span> + metadata.topic() + <span class="string">&quot;, 分区: &quot;</span> + metadata.partition());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            Thread.sleep(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 3. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注：Kafka默认的分区分配策略就是Range+CooperativeSticky,所以不需要修改策略。</strong></p>
<p>（4）观看3个消费者分别消费哪些分区的数据。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941459.png" alt="image-20230402153715657"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941460.png" alt="image-20230402153743381"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941461.png" alt="image-20230402153813830"></p>
<p>假设上述消费者，</p>
<ul>
<li>消费0、1、2分区的为0号消费者。</li>
<li>消费3、4分区的为1号消费者。</li>
<li>消费5、6分区的为2号消费者。</li>
</ul>
<p><strong>3）Range分区分配再平衡案例</strong></p>
<p>（1）停掉0号消费者（消费0、1、2分区的消费者），快速重新发送消费观看结果（45s以内，越快越好）</p>
<ul>
<li>
<p>1号消费者：消费到3、4号分区数据</p>
</li>
<li>
<p>2号消费者：消费到5、6号分区数据</p>
</li>
<li>
<p>0号消费者的任务会<strong>整体分配到</strong>1号消费者或者2号消费者。</p>
</li>
</ul>
<blockquote>
<p>说明：0号消费者挂掉后，消费者组需要按照超时时间45s来判断它是否退出，所以需要等待，时间到了45s后，判断它真的退出就会把任务分配给其他消费者执行。</p>
</blockquote>
<p>（2）再次重新发送消费观看结果（45s以后）</p>
<ul>
<li>
<p>1号消费者：消费到0、1、2、3号分区数据。</p>
</li>
<li>
<p>2号消费者：消费到4、5、6号分区数据。</p>
</li>
</ul>
<blockquote>
<p>说明：消费者0已经被踢出消费者组，所以重新按照range方式分配。</p>
</blockquote>
<h4 id="RoundRobin以及再平衡">RoundRobin以及再平衡</h4>
<p><strong>1）RoundRobin分区策略原理</strong></p>
<p><strong>RoundRobin是根据所有topic来分配分区的</strong></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941462.png" alt="image-20230402155712072"></p>
<p><strong>2）RoundRobin分区分配策略案例</strong></p>
<p>（1）依次在 CustomConsumer、CustomConsumer1、CustomConsumer2 三个消费者代码中修改分区分配策略为 RoundRobin。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 修改分区分配策略</span></span><br><span class="line">properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;</span>);   </span><br></pre></td></tr></table></figure>
<p>（2）重启3个消费者，重复发送消费的步骤，观看分区结果。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941463.png" alt="image-20230402184447295"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941464.png" alt="image-20230402184527642"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941465.png" alt="image-20230402184549195"></p>
<p><strong>3）RoundRobin分区分配再平衡案例</strong></p>
<p>（1）停止掉0号消费者，快速重新发送消息观看结果（45s以内，越快越好）。</p>
<p>1号消费者：消费到2、5号分区数据</p>
<p>2号消费者：消费到4、1号分区数据</p>
<p>0号消费者的任务会按照RoundRobin的方式，把数据轮询分成0、6和3号分区数据，分别由1号消费者或者2号消费者消费。</p>
<blockquote>
<p>说明：0号消费者挂掉后，消费者组需要按照超时时间45s来判断它是否退出，所以需要等待，时间到了45s后，判断它真的退出就会把任务分配给其他broker执行。</p>
</blockquote>
<p>（2）再次重新发送消息观看结果(45s以后)。</p>
<p>1号消费者：消费到0、2、4、6号分区数据</p>
<p>2号消费者：消费到1、3、5号分区数据</p>
<blockquote>
<p>说明：消费者0己经被踢出消费者组，所以重新按照RoundRobin方式分配。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941466.png" alt="image-20230402185409213"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941467.png" alt="image-20230402185504506"></p>
<h4 id="Sticky-以及再平衡">Sticky 以及再平衡</h4>
<p><strong>粘性分区定义</strong>：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。</p>
<p>粘性分区是Kafka从0.11.x版本开始引入这种分配策略，<strong>首先会尽量均衡的放置分区到消费者上面</strong>，在出现同一消费者组内消费者出现问题的时候，<strong>会尽量保持原有分配的分区不变化</strong>。</p>
<p><strong>1）需求</strong></p>
<p>​	设置主题为first，7个分区；准备3个消费者，采用粘性分区策略，并进行消费，观察消费分配情况。然后再停止其中一个消费者，再次观察消费分配情况。</p>
<p><strong>2）步骤</strong></p>
<p>（1）修改分区分配策略为粘性</p>
<p>注意：3个消费者都应该注释掉，之后重启3个消费者，如果出现报错，全部停止，等会再重启，或者修改为全新的消费者组。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 修改分区分配策略</span></span><br><span class="line">properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>（2）使用同样的生产者发送500条消息。</p>
<p>可以看到会尽量保持分区的个数，近似划分分区。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941468.png" alt="image-20230402193849773"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941469.png" alt="image-20230402193906902"></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941470.png" alt="image-20230402193924981"></p>
<blockquote>
<p>注：以上消费者消费的分区是随机分配的。可以重启并重新发送以验证它的随机性。</p>
</blockquote>
<p><strong>3）Sticky分区分配再平衡案例</strong></p>
<p>（1）停止掉0号消费者，快速重新发送消息观看结果（45s以内，越快越好）</p>
<p>1号消费者：消费1、4号分区数据</p>
<p>2号消费者：消费2、5号分区数据</p>
<p>0号消费者的任务会按照粘性规则，尽可能均衡的随机分成0和1号分区数据，分别由1号消费者或者2号消费者消费。</p>
<blockquote>
<p>说明：0号消费者挂掉后，消费者组需要按照超时时间45s来判断它是否退出，所以需要等待，时间到了45s后，判断它真的退出就会把任务分配给其他broker执行。</p>
</blockquote>
<p>（2）再次重新发送消息观看结果(45s以后)。</p>
<p>1号消费者：消费到2、3、5号分区数据。</p>
<p>2号消费者：消费到0、1、4、6号分区数据。</p>
<blockquote>
<p>说明：消费者0己经被踢出消费者组，所以重新按照粘性方式分配。</p>
</blockquote>
<h3 id="offset-位移">offset 位移</h3>
<h4 id="offset的默认维护位置">offset的默认维护位置</h4>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941471.png" alt="image-20230402195027603"></p>
<p><code>__consumer_offsets</code>主题里面采用<code>key</code>和<code>value</code>的方式存储数据。<code>key</code>是 group.id+topic+<br>
分区号，<code>value</code>就是当前offset的值。每隔一段时间，kafka内部会对这个topic进行<code>compact</code>，也就是每个 group.id+topic+分区号 就保留最新数据。</p>
<p><strong>1）消费offset案例</strong></p>
<p>（0）思想：consumeroffsets为Kafka中的topic,那就可以通过消费者进行消费。</p>
<p>（1）在配置文件<code>config/consumer.properties</code>中添加配置<code>exclude.internal.topics=false</code>。默认是tue，表示不能消费系统主题。为了查看该系统主题数据，所以该参数修改为false。不用重启</p>
<p>（2）采用命令行方式，创建一个新的topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --bootstrap-server centos_02:9092 --create --topic atguigu --partitions 2 --replication-factor 2</span><br></pre></td></tr></table></figure>
<p>（3）启动生产者往atguigu生产数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_02 kafka]# bin/kafka-console-producer.sh --topic atguigu --bootstrap-server centos_02:9092</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（4）启动消费者消费atguigu数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_03 kafka]# bin/kafka-console-consumer.sh --bootstrap-server centos_02:9092 --topic atguigu --group test</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>注：指定消费者组名称，可以更好地观察数据的存储位置（key 是 group.id+topic+分区号）</strong></p>
<p>（5）查看消费者消费主题<code>__consumer_offsets</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@centos_04 kafka]# bin/kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server centos_02:9092 --consumer.config config/consumer.properties --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941472.png" alt="image-20230403145915979"></p>
<h4 id="自动提交-offset">自动提交 offset</h4>
<p>为了使我们能够专注于自己的业务逻辑，<strong>Kafka提供了自动提交<code>offset</code>的功能</strong>。</p>
<p>自动提交<code>offset</code>的相关参数：</p>
<ul>
<li><code>enable.auto.commit</code>：是否开启自动提交offset功能，默认是true</li>
<li><code>auto.commit.interval.ms</code>：自动提交offset的时间间隔，默认是5s</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941473.png" alt="image-20230403150118877"></p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>enable.auto.commit</code></td>
<td>默认值为true，消费者会自动周期性地向服务器提交偏移量。</td>
</tr>
<tr>
<td><code>auto.commit.interval.ms</code></td>
<td>如果设置了enable.auto.commit的值为tue,则该值定义了消费者偏移量向Kafka提交的频率，默认5s。</td>
</tr>
</tbody>
</table>
<p><strong>1）消费者自动提交offset</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerAutoOffset</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">// 连接 bootstrap.servers</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092,192.168.179.132:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 反序列化</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line">        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;</span>);</span><br><span class="line">        <span class="comment">// groupid</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test4&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 自动提交, 默认就是true</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">true</span>);</span><br><span class="line">        <span class="comment">// 提交时间间隔, 单位ms</span></span><br><span class="line">        properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建一个消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">        <span class="comment">// 2. 定义主题 first</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line">        <span class="comment">// 3. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置每秒拉取一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 打印拉取到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="手动提交offset">手动提交offset</h4>
<p>虽然自动提交offset十分简单便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此kafka还提供了手动提交offset的API。</p>
<p>手动提交offset的方法有两种：分别是**<code>commitSync</code>（同步提交）<strong>和 <strong><code>commitAsync</code>（异步提交）</strong>。两者的相同点是：<strong>都会将本次提交的一批数据最高的偏移量提交</strong>；不同点是：<strong>同步提交阻塞当前线程</strong>，一直到提交成功，并且会自动失败重试(由不可控因素导致，也会出现提交失败)。而</strong>异步提交则没有失败重试机制，故有可能提交失败**。</p>
<ul>
<li><code>commitSync</code>（同步提交）：必须等待offset提交完毕，再去消费下一批数据。</li>
<li><code>commitAsync</code>（异步提交）：发送完提交offset请求后，就开始消费下一批数据了。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941474.png" alt="image-20230403153423101"></p>
<p><strong>1）同步提交offset</strong></p>
<p>由于同步提交offset有失败重试机制，故更加可靠，但是由于一直等待提交结果，提交的效率比较低。</p>
<p>以下为同步提交offset的示例。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerByHandSync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">// 连接 bootstrap.servers</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092,192.168.179.132:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 反序列化</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line">        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;</span>);</span><br><span class="line">        <span class="comment">// groupid</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test5&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 手动提交</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建一个消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">        <span class="comment">// 2. 定义主题 first</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line">        <span class="comment">// 3. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置每秒拉取一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 打印拉取到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 手动提交offset, 同步提交</span></span><br><span class="line">            kafkaConsumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>2）异步提交offset</strong></p>
<p>虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。因此更多的情况下，会选用异步提交offset的方式。</p>
<p>以下为异步提交offset的示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerByHandAsync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">// 连接 bootstrap.servers</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092,192.168.179.132:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 反序列化</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line">        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;</span>);</span><br><span class="line">        <span class="comment">// groupid</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test5&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 手动提交</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建一个消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">        <span class="comment">// 2. 定义主题 first</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line">        <span class="comment">// 3. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置每秒拉取一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 打印拉取到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 手动提交offset, 异步提交</span></span><br><span class="line">            kafkaConsumer.commitAsync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在生产环境中，通常采用异步提交的方式。</p>
</blockquote>
<h4 id="指定offset消费">指定offset消费</h4>
<p><code>auto.offset.reset</code> = earliest | latest | none，默认是latest。</p>
<p>当Kafka中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办？</p>
<p>（1）<code>earliest</code>：自动将偏移量重置为最早的偏移量，–from-beginning。</p>
<p>（2）<code>latest</code>（默认值）：自动将偏移量重置为最新偏移量。</p>
<p>（3）<code>none</code>：如果未找到消费者组的先前偏移量，则向消费者抛出异常。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941475.png" alt="image-20230403165208521"></p>
<p>（4）任意指定offset位移开始消费</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerSeek</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092,192.168.179.132:9092&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test10&quot;</span>);</span><br><span class="line">        <span class="comment">// 1. 创造消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">        <span class="comment">// 2. 订阅主题</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取当前分配给此消费者的分区集合</span></span><br><span class="line">        Set&lt;TopicPartition&gt; assignment = kafkaConsumer.assignment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 保证分区分配方案已经制定完毕</span></span><br><span class="line">        <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">            kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            assignment = kafkaConsumer.assignment();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 在对应分区中指定消费的offset</span></span><br><span class="line">        <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">            kafkaConsumer.seek(topicPartition, <span class="number">35000</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>主要添加了以下代码：</strong></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941476.png" alt="image-20230403185616021"></p>
<p>打印结果如下：可以看到只消费了offset&gt;=35000的数据。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941477.png" alt="image-20230403185218668"></p>
<h4 id="指定时间消费">指定时间消费</h4>
<p>需求：在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间 消费前一天的数据，怎么处理？</p>
<p>操作步骤：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerForTime</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0. 配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.179.131:9092,192.168.179.132:9092&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test10&quot;</span>);</span><br><span class="line">        <span class="comment">// 1. 创造消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">        <span class="comment">// 2. 订阅主题</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取当前分配给此消费者的分区集合</span></span><br><span class="line">        Set&lt;TopicPartition&gt; assignment = kafkaConsumer.assignment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 保证分区分配方案已经制定完毕</span></span><br><span class="line">        <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">            kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            assignment = kafkaConsumer.assignment();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 希望把时间转换为对应的offset</span></span><br><span class="line">        HashMap&lt;TopicPartition, Long&gt; topicPartitionLongHashMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">// 封装对应集合</span></span><br><span class="line">        <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">            <span class="comment">// 指定时间为一天前</span></span><br><span class="line">            topicPartitionLongHashMap.put(topicPartition, System.currentTimeMillis() - <span class="number">1</span> * <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 通过时间获取到对应的offset</span></span><br><span class="line">        Map&lt;TopicPartition, OffsetAndTimestamp&gt; topicPartitionOffsetAndTimestampMap = kafkaConsumer.offsetsForTimes(topicPartitionLongHashMap);</span><br><span class="line">        <span class="comment">// 在对应分区中指定消费的offset</span></span><br><span class="line">        <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">            <span class="type">OffsetAndTimestamp</span> <span class="variable">offsetAndTimestamp</span> <span class="operator">=</span> topicPartitionOffsetAndTimestampMap.get(topicPartition);</span><br><span class="line">            <span class="keyword">if</span> (offsetAndTimestamp != <span class="literal">null</span>) &#123;</span><br><span class="line">                kafkaConsumer.seek(topicPartition, offsetAndTimestamp.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 消费该主题数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>主要看以下代码：</strong></p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941478.png" alt="image-20230403190932257"></p>
<h4 id="漏消费和重复消费">漏消费和重复消费</h4>
<ul>
<li>
<p><strong>重复消费</strong>：已经消费了数据，但是offset没提交。可能会导致同一个数据被重复消费。</p>
</li>
<li>
<p><strong>漏消费</strong>：先提交offset后消费，有可能会造成数据的漏消费。</p>
</li>
</ul>
<p><strong>1）可能导致重复消费的场景</strong></p>
<ul>
<li><strong>自动提交offset可能引起重复消费</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941479.png" alt="image-20230403191331379"></p>
<p><strong>2）可能导致漏消费的场景</strong></p>
<ul>
<li><strong>消费者先提交offset后消费数据</strong>。设置offset为手动提交，当offset被提交时，数据还在内存中未落盘，此时刚好消费者线程挂掉，那么offset已经提交，但是数据未处理，导致这部分内存中的数据丢失。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941480.png" alt="image-20230403191629056"></p>
<hr/>
<p><strong>如何做到既不漏消费也不重复消费？看下节消费者事务</strong></p>
<blockquote>
<p><strong>如果面试官问：如何做到数据的精确一次性消费？</strong></p>
<p>那么需要从以下方面回答：</p>
<ul>
<li>生产端 —&gt;  Kafka集群</li>
<li>Kafka集群  —&gt;  消费者端</li>
<li>消费者端  —&gt;  下游框架</li>
</ul>
<p>只有上述三个环节都不出现问题，才能做到精确一次性消费数据。</p>
</blockquote>
<h3 id="生产经验——消费者事务">生产经验——消费者事务</h3>
<p>如果想完成Consumer端的精准一次性消费，那么需要**<code>Kafka</code>消费端将消费过程和提交<code>offset</code>过程做原子绑定**。此时我们需要将<code>Kafka</code>的<code>offset</code>保存到支持事务的自定义介质（比如MySQL)。这部分知识会在后续项目部分涉及。</p>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941481.png" alt="image-20230403192200304"></p>
<h3 id="生产经验——数据积压（消费者如何提高吞吐量）">生产经验——数据积压（消费者如何提高吞吐量）</h3>
<p>出现数据积压的原因可能有两种：</p>
<ul>
<li>
<p>如果是 Kafka 消费能力不足，则可以考虑<strong>增加Topic的分区数</strong>，并且<strong>同时提升消费组的消费者<br>
数量</strong>，<strong>消费者数=分区数</strong>。（两者缺一不可）</p>
</li>
<li>
<p>如果是下游的数据处理不及时：<strong>提高每批次拉取的数量</strong>。批次拉取数据过少(拉取数据 / 处理时间 &lt; 生产速度)，使处理的数据小于生产的数据，也会造成数据积压。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hugh-98/PicGo/main/img/202304031941482.png" alt="image-20230403192920675"></p>
<blockquote>
<p>大致总结 数据积压的解决办法：</p>
<ul>
<li>增加分区、增加消费者个数</li>
<li>生产端 -&gt; Kafka集群 有4个参数可以调节</li>
<li>消费端 有2个参数</li>
</ul>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://hugh-98.github.io">Hugh</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://hugh-98.github.io/2023/04/03/Kafka%E5%9F%BA%E7%A1%80/">https://hugh-98.github.io/2023/04/03/Kafka基础/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://hugh-98.github.io" target="_blank">Hugh的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Kafka%E5%85%A5%E9%97%A8/">Kafka入门</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/03/20/Git/" title="Git入门"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Git入门</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Hugh</div><div class="author-info__description">Life is like a box of chocolates, you never know what you're going to get.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/hugh-98"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">若在文章中无法显示图片，则需先用科学上网后才能看到。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">Kafka 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97"><span class="toc-number">1.2.</span> <span class="toc-text">消息队列</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.2.1.</span> <span class="toc-text">传统消息队列的应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.2.2.</span> <span class="toc-text">消息队列的两种模式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-number">1.3.</span> <span class="toc-text">Kafka 基础架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-number">2.</span> <span class="toc-text">Kafka 快速入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-number">2.1.</span> <span class="toc-text">安装部署</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92"><span class="toc-number">2.1.1.</span> <span class="toc-text">集群规划</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2"><span class="toc-number">2.1.2.</span> <span class="toc-text">集群部署</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E8%84%9A%E6%9C%AC%E7%AE%A1%E7%90%86kafka%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E5%81%9C%E6%AD%A2"><span class="toc-number">2.1.3.</span> <span class="toc-text">利用脚本管理kafka的启动和停止</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.</span> <span class="toc-text">Kafka 命令行操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E9%A2%98%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.1.</span> <span class="toc-text">主题命令行操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.2.</span> <span class="toc-text">生产者命令行操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.3.</span> <span class="toc-text">消费者命令行操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-number">3.</span> <span class="toc-text">Kafka 生产者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.</span> <span class="toc-text">生产者消息发送流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%91%E9%80%81%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.1.</span> <span class="toc-text">发送原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E5%88%97%E8%A1%A8"><span class="toc-number">3.1.2.</span> <span class="toc-text">生产者重要参数列表</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81-API"><span class="toc-number">3.2.</span> <span class="toc-text">异步发送 API</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%99%AE%E9%80%9A%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81"><span class="toc-number">3.2.1.</span> <span class="toc-text">普通异步发送</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%A6%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E7%9A%84%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81"><span class="toc-number">3.2.2.</span> <span class="toc-text">带回调函数的异步发送</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E5%8F%91%E9%80%81API"><span class="toc-number">3.3.</span> <span class="toc-text">同步发送API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E5%88%86%E5%8C%BA"><span class="toc-number">3.4.</span> <span class="toc-text">生产者分区</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E5%A5%BD%E5%A4%84"><span class="toc-number">3.4.1.</span> <span class="toc-text">分区好处</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E7%9A%84%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="toc-number">3.4.2.</span> <span class="toc-text">生产者发送消息的分区策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-number">3.4.3.</span> <span class="toc-text">自定义分区器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E7%94%9F%E4%BA%A7%E8%80%85%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F"><span class="toc-number">3.5.</span> <span class="toc-text">生产经验——生产者如何提高吞吐量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E5%8F%AF%E9%9D%A0%E6%80%A7"><span class="toc-number">3.6.</span> <span class="toc-text">生产经验——数据可靠性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D"><span class="toc-number">3.7.</span> <span class="toc-text">生产经验——数据去重</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E8%AF%AD%E4%B9%89"><span class="toc-number">3.7.1.</span> <span class="toc-text">数据传递语义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-number">3.7.2.</span> <span class="toc-text">幂等性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E4%BA%8B%E5%8A%A1"><span class="toc-number">3.7.3.</span> <span class="toc-text">生产者事务</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%9C%89%E5%BA%8F"><span class="toc-number">3.8.</span> <span class="toc-text">生产经验——数据有序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E4%B9%B1%E5%BA%8F"><span class="toc-number">3.9.</span> <span class="toc-text">生产经验——数据乱序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-Broker"><span class="toc-number">4.</span> <span class="toc-text">Kafka Broker</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-Broker-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">Kafka Broker 工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Zookeeper-%E5%AD%98%E5%82%A8%E7%9A%84Kafka%E4%BF%A1%E6%81%AF"><span class="toc-number">4.1.1.</span> <span class="toc-text">Zookeeper 存储的Kafka信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka-Broker-%E6%80%BB%E4%BD%93%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.2.</span> <span class="toc-text">Kafka Broker 总体工作流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E8%8A%82%E7%82%B9%E6%9C%8D%E5%BD%B9%E5%92%8C%E9%80%80%E5%BD%B9"><span class="toc-number">4.2.</span> <span class="toc-text">生产经验——节点服役和退役</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%8D%E5%BD%B9%E6%96%B0%E8%8A%82%E7%82%B9"><span class="toc-number">4.2.1.</span> <span class="toc-text">服役新节点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%80%E5%BD%B9%E6%97%A7%E8%8A%82%E7%82%B9"><span class="toc-number">4.2.2.</span> <span class="toc-text">退役旧节点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-%E5%89%AF%E6%9C%AC"><span class="toc-number">4.3.</span> <span class="toc-text">Kafka 副本</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">4.3.1.</span> <span class="toc-text">副本基本信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Leader%E9%80%89%E4%B8%BE%E6%B5%81%E7%A8%8B"><span class="toc-number">4.3.2.</span> <span class="toc-text">Leader选举流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Leader%E5%92%8CFollower%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82"><span class="toc-number">4.3.3.</span> <span class="toc-text">Leader和Follower故障处理细节</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E5%88%86%E9%85%8D"><span class="toc-number">4.3.4.</span> <span class="toc-text">分区副本分配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E6%89%8B%E5%8A%A8%E8%B0%83%E6%95%B4%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E5%AD%98%E5%82%A8"><span class="toc-number">4.3.5.</span> <span class="toc-text">生产经验——手动调整分区副本存储</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94Leader-Partition%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1"><span class="toc-number">4.3.6.</span> <span class="toc-text">生产经验——Leader Partition负载平衡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E5%A2%9E%E5%8A%A0%E5%89%AF%E6%9C%AC%E5%9B%A0%E5%AD%90"><span class="toc-number">4.3.7.</span> <span class="toc-text">生产经验——增加副本因子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8"><span class="toc-number">4.4.</span> <span class="toc-text">文件存储</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6"><span class="toc-number">4.4.1.</span> <span class="toc-text">文件存储机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86%E7%AD%96%E7%95%A5"><span class="toc-number">4.4.2.</span> <span class="toc-text">文件清理策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE"><span class="toc-number">4.5.</span> <span class="toc-text">Kafka如何高效读写数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-number">5.</span> <span class="toc-text">Kafka消费者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E6%B6%88%E8%B4%B9%E6%96%B9%E5%BC%8F"><span class="toc-number">5.1.</span> <span class="toc-text">Kafka消费方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.</span> <span class="toc-text">Kafka消费者工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E6%80%BB%E4%BD%93%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.1.</span> <span class="toc-text">消费者总体工作流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%8E%9F%E7%90%86"><span class="toc-number">5.2.2.</span> <span class="toc-text">消费者组原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.3.</span> <span class="toc-text">消费者组初始化流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E8%AF%A6%E7%BB%86%E6%B6%88%E8%B4%B9%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.4.</span> <span class="toc-text">消费者组详细消费流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0"><span class="toc-number">5.2.5.</span> <span class="toc-text">消费者重要参数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85API"><span class="toc-number">5.3.</span> <span class="toc-text">消费者API</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8B%AC%E7%AB%8B%E6%B6%88%E8%B4%B9%E8%80%85%E6%A1%88%E4%BE%8B%EF%BC%88%E8%AE%A2%E9%98%85%E4%B8%BB%E9%A2%98%EF%BC%89"><span class="toc-number">5.3.1.</span> <span class="toc-text">独立消费者案例（订阅主题）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8B%AC%E7%AB%8B%E6%B6%88%E8%B4%B9%E8%80%85%E6%A1%88%E4%BE%8B%EF%BC%88%E8%AE%A2%E9%98%85%E5%88%86%E5%8C%BA%EF%BC%89"><span class="toc-number">5.3.2.</span> <span class="toc-text">独立消费者案例（订阅分区）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E6%A1%88%E4%BE%8B"><span class="toc-number">5.3.3.</span> <span class="toc-text">消费者组案例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E5%88%86%E5%8C%BA%E7%9A%84%E5%88%86%E9%85%8D%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">5.4.</span> <span class="toc-text">生产经验——分区的分配以及再平衡</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Range-%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">5.4.1.</span> <span class="toc-text">Range 以及再平衡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RoundRobin%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">5.4.2.</span> <span class="toc-text">RoundRobin以及再平衡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Sticky-%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">5.4.3.</span> <span class="toc-text">Sticky 以及再平衡</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#offset-%E4%BD%8D%E7%A7%BB"><span class="toc-number">5.5.</span> <span class="toc-text">offset 位移</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#offset%E7%9A%84%E9%BB%98%E8%AE%A4%E7%BB%B4%E6%8A%A4%E4%BD%8D%E7%BD%AE"><span class="toc-number">5.5.1.</span> <span class="toc-text">offset的默认维护位置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%8F%90%E4%BA%A4-offset"><span class="toc-number">5.5.2.</span> <span class="toc-text">自动提交 offset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8F%90%E4%BA%A4offset"><span class="toc-number">5.5.3.</span> <span class="toc-text">手动提交offset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%87%E5%AE%9Aoffset%E6%B6%88%E8%B4%B9"><span class="toc-number">5.5.4.</span> <span class="toc-text">指定offset消费</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A%E6%97%B6%E9%97%B4%E6%B6%88%E8%B4%B9"><span class="toc-number">5.5.5.</span> <span class="toc-text">指定时间消费</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="toc-number">5.5.6.</span> <span class="toc-text">漏消费和重复消费</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E6%B6%88%E8%B4%B9%E8%80%85%E4%BA%8B%E5%8A%A1"><span class="toc-number">5.6.</span> <span class="toc-text">生产经验——消费者事务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%EF%BC%88%E6%B6%88%E8%B4%B9%E8%80%85%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%89"><span class="toc-number">5.7.</span> <span class="toc-text">生产经验——数据积压（消费者如何提高吞吐量）</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/04/03/Kafka%E5%9F%BA%E7%A1%80/" title="Kafka基础——入门">Kafka基础——入门</a><time datetime="2023-04-03T11:46:27.000Z" title="发表于 2023-04-03 19:46:27">2023-04-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/20/Git/" title="Git入门">Git入门</a><time datetime="2023-03-20T11:41:20.000Z" title="发表于 2023-03-20 19:41:20">2023-03-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/15/Shell%E7%BC%96%E7%A8%8B/" title="Shell编程">Shell编程</a><time datetime="2023-03-15T12:47:23.000Z" title="发表于 2023-03-15 20:47:23">2023-03-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/11/Linux%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" title="Linux基础（二）常用命令">Linux基础（二）常用命令</a><time datetime="2023-03-11T11:53:20.000Z" title="发表于 2023-03-11 19:53:20">2023-03-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/07/Linux%E5%9F%BA%E7%A1%80/" title="Linux基础（一）">Linux基础（一）</a><time datetime="2023-03-07T02:07:46.000Z" title="发表于 2023-03-07 10:07:46">2023-03-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By Hugh</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://hugh-98.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'mOxPmpzRdaSEBRH4FPLh3IOA-gzGzoHsz',
      appKey: 'KXhA9ey1dBoGtsiuDmz48WWk',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>